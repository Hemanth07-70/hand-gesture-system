# Hand Gesture Monitoring System

Real-time hand gesture recognition: train a model on your gestures (e.g. ğŸ‘ Thumbs Up, ğŸ˜Š Happy, ğŸ˜¢ Sad) and see the corresponding emoji on screen.

## Features

- **Login / Register** â€“ Simple auth, then redirect to monitor
- **Live webcam** â€“ MediaPipe hand detection and landmark extraction
- **Data collection** â€“ Record samples per gesture (script or API)
- **Train model** â€“ Random Forest on 21Ã—3 hand landmarks
- **Real-time prediction** â€“ Gesture â†’ emoji display with confidence
- **Retrain** â€“ Button on monitor page to retrain from collected data

## Gestures (default)

| Gesture     | Emoji |
|------------|-------|
| thumbs_up  | ğŸ‘    |
| thumbs_down| ğŸ‘    |
| peace      | âœŒï¸    |
| ok         | ğŸ‘Œ    |
| wave       | ğŸ‘‹    |
| happy      | ğŸ˜Š    |
| sad        | ğŸ˜¢    |
| crying     | ğŸ˜­    |
| rock       | ğŸ¤˜    |
| fist       | âœŠ    |
| open_palm  | ğŸ–ï¸    |

## Requirements

- Python 3.9+
- Webcam (720p+ recommended)
- 8GB RAM (16GB recommended)

## Setup

```bash
cd hand-gesture-system
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate
pip install -r requirements.txt
cp .env.example .env
# Edit .env and set SECRET_KEY if you like
```

## Run

```bash
python app.py
```

Open [http://127.0.0.1:5000](http://127.0.0.1:5000). Register or login, then youâ€™re on the monitor page with live feed and emoji result.

## Collect data and train

1. **Collect samples** (200â€“500 per gesture, different angles/lighting):

   ```bash
   python scripts/collect_data.py
   ```
   Enter gesture name (e.g. `thumbs_up`, `happy`, `sad`). Point your hand at the webcam and press **SPACE** to save a batch. Repeat. Press **Q** to quit.

2. **Train**  
   On the monitor page, click **Retrain model**. The app uses `dataset/*.npz` to train a classifier and saves it under `model/saved_models/`.

## Project layout

```
hand-gesture-system/
â”œâ”€â”€ app.py              # Flask app (login, monitor, video_feed, API)
â”œâ”€â”€ config.py           # Paths, gestureâ†’emoji map
â”œâ”€â”€ auth/               # Registration and login
â”œâ”€â”€ detection/          # Camera, MediaPipe, pipeline
â”œâ”€â”€ data/               # Collection, preprocessing, users
â”œâ”€â”€ model/              # Train and predict
â”œâ”€â”€ templates/          # Login, register, monitor
â”œâ”€â”€ static/             # CSS, JS, img (vibrant background)
â”œâ”€â”€ dataset/            # Per-gesture .npz files
â”œâ”€â”€ scripts/            # collect_data.py
â””â”€â”€ model/saved_models/ # Trained classifier + label encoder
```

## Optional: vibrant background image

Replace `static/img/background.svg` with a JPG/PNG for a custom background. The UI uses a gradient overlay; the CSS references `background.svg` by default.
